\relax 
\providecommand\hyper@newdestlabel[2]{}
\providecommand{\transparent@use}[1]{}
\citation{santos_et_al2005,ali_et_al2008,deOliveiraCosta_et_al2008,jelic_et_al2008,bernardi_et_al2009,bernardi_et_al2010,ghosh_et_al2011,pober_et_al2013b,bernardi_et_al2013,dillon_et_al2014,kohn_et_al2016}
\citation{ali_et_al2015}
\citation{ali_et_al2015}
\citation{Paciga2013}
\citation{Patil2016}
\citation{Jacobs2016}
\@writefile{toc}{\contentsline {chapter}{\numberline {2}Power Spectrum Methods}{22}{chapter.2}}
\@writefile{lof}{\addvspace {10\p@ }}
\@writefile{lot}{\addvspace {10\p@ }}
\newlabel{c.PSmethods}{{2}{22}{Power Spectrum Methods}{chapter.2}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.1}Power Spectrum Themes and Techniques}{22}{section.2.1}}
\citation{parsons_et_al2016}
\citation{liu_tegmark2011}
\citation{dillon_et_al2013a}
\citation{liu_et_al2014a}
\citation{liu_et_al2014b}
\citation{dillon_et_al2014}
\citation{dillon_et_al2015}
\citation{andrae2010}
\citation{keating_et_al2016}
\citation{ade_et_al2008}
\citation{chiang_et_al2010}
\citation{bischoff_et_al2011}
\citation{das_et_al2011b}
\citation{araujo_et_al2012}
\citation{crites_et_al2015}
\citation{ade_et_al2016}
\citation{ade_et_al2017}
\citation{sherwin_et_al2017}
\citation{ali_et_al2015}
\citation{liu_tegmark2011}
\citation{dillon_et_al2013a}
\citation{liu_et_al2014a}
\citation{liu_et_al2014b}
\citation{trott_et_al2012}
\citation{dillon_et_al2014}
\citation{dillon_et_al2015}
\citation{switzer_et_al2015}
\citation{trott_et_al2016}
\citation{parsons_et_al2012b}
\@writefile{toc}{\contentsline {section}{\numberline {2.2}Signal Loss Toy Model}{25}{section.2.2}}
\newlabel{sec:SiglossOverview}{{2.2}{25}{Signal Loss Toy Model}{section.2.2}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.1}The Quadratic Estimator Method}{25}{subsection.2.2.1}}
\newlabel{sec:QE}{{2.2.1}{25}{The Quadratic Estimator Method}{subsection.2.2.1}{}}
\citation{tegmark_et_al1997a,bond_et_al1998}
\citation{liu_tegmark2011}
\newlabel{eq:OQE}{{2.3}{26}{The Quadratic Estimator Method}{equation.2.2.3}{}}
\newlabel{eq:OQEQuadratic}{{2.5}{26}{The Quadratic Estimator Method}{equation.2.2.5}{}}
\newlabel{eq:OQELinear}{{2.6}{26}{The Quadratic Estimator Method}{equation.2.2.6}{}}
\newlabel{eq:super_unbiased}{{2.7}{26}{The Quadratic Estimator Method}{equation.2.2.7}{}}
\citation{chang_et_al2010}
\citation{switzer_et_al2015}
\newlabel{eq:qhat}{{2.8}{27}{The Quadratic Estimator Method}{equation.2.2.8}{}}
\newlabel{eq:phat}{{2.9}{27}{The Quadratic Estimator Method}{equation.2.2.9}{}}
\newlabel{eq:Wpplusbias}{{2.10}{27}{The Quadratic Estimator Method}{equation.2.2.10}{}}
\citation{ali_et_al2015}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.2}Empirical Inverse Covariance Weighting}{28}{subsection.2.2.2}}
\newlabel{sec:toymodel}{{2.2.2}{28}{Empirical Inverse Covariance Weighting}{subsection.2.2.2}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.1}{\ignorespaces Our toy model dataset to which we apply different weighting schemes to in order to investigate signal loss. We model a mock foreground-only visibility with a sinusoid signal that varies smoothly in time and frequency. We model a mock visibility of an EoR signal as a random Gaussian signal. We add the two together to form $\textbf  {x} = \textbf  {x}_{\rm  FG} + \textbf  {x}_{\rm  EoR}$. Real parts are shown here.\relax }}{29}{figure.caption.14}}
\newlabel{fig:toy_sigloss1}{{2.1}{29}{Our toy model dataset to which we apply different weighting schemes to in order to investigate signal loss. We model a mock foreground-only visibility with a sinusoid signal that varies smoothly in time and frequency. We model a mock visibility of an EoR signal as a random Gaussian signal. We add the two together to form $\textbf {x} = \textbf {x}_{\rm FG} + \textbf {x}_{\rm EoR}$. Real parts are shown here.\relax }{figure.caption.14}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.2}{\ignorespaces The estimated covariance matrices (top row) and inverse covariance-weighted data (bottom row) for FG only (left), EoR only (middle), and FG + EoR (right). Real parts are shown here.\relax }}{30}{figure.caption.15}}
\newlabel{fig:toy_sigloss12}{{2.2}{30}{The estimated covariance matrices (top row) and inverse covariance-weighted data (bottom row) for FG only (left), EoR only (middle), and FG + EoR (right). Real parts are shown here.\relax }{figure.caption.15}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.3}{\ignorespaces Resulting power spectrum estimates for the toy model simulation described in Chapter \ref  {sec:toymodel} --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). The power spectrum of the foregrounds peaks at a $k$-mode based on the frequency of the sinusoid used to create the mock FG signal. In the two panels, we compare using empirically estimated inverse covariance weighting where $\textbf  {C}$ is derived from the data (left), and projecting out the zeroth eigenmode only (right). In the former case, signal loss arises from the coupling of the eigenmodes of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \textbf  {C}$}\mathaccent "0362{\textbf  {C}}$ to the data. There is negligible signal loss when all eigenmodes besides the foreground one are no longer correlated with the data. \relax }}{31}{figure.caption.16}}
\newlabel{fig:toy_sigloss3}{{2.3}{31}{Resulting power spectrum estimates for the toy model simulation described in Chapter \ref {sec:toymodel} --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). The power spectrum of the foregrounds peaks at a $k$-mode based on the frequency of the sinusoid used to create the mock FG signal. In the two panels, we compare using empirically estimated inverse covariance weighting where $\textbf {C}$ is derived from the data (left), and projecting out the zeroth eigenmode only (right). In the former case, signal loss arises from the coupling of the eigenmodes of $\widehat {\textbf {C}}$ to the data. There is negligible signal loss when all eigenmodes besides the foreground one are no longer correlated with the data. \relax }{figure.caption.16}{}}
\citation{parsons_et_al2016}
\citation{dodelson_schneider2013,taylor_joachimi_etal2014}
\citation{hartlap_et_al2007}
\citation{padmanabhan_et_al2016}
\citation{pearson_samushia2016}
\citation{paz_sanchez2015}
\citation{sellentin_heavens2016}
\citation{pope_szapudi2008,joachimi_2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.3}Fringe-Rate Filtering}{32}{subsection.2.2.3}}
\newlabel{sec:toymodel_frf}{{2.2.3}{32}{Fringe-Rate Filtering}{subsection.2.2.3}{}}
\newlabel{eq:converge}{{2.12}{32}{Fringe-Rate Filtering}{equation.2.2.12}{}}
\newlabel{eq:converge_eig}{{2.13}{33}{Fringe-Rate Filtering}{equation.2.2.13}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.4}{\ignorespaces The convergence level, as defined by Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:converge}\unskip \@@italiccorr )}}, of empirically estimated covariances of mock EoR signals with different numbers of independent samples. In red, the mock EoR signal is comprised entirely of independent samples (100 of them). Subsequent colors show time-averaged signals. As the number of realizations increases, we see that the empirical covariances approach the true covariances. With more independent samples, the quicker an empirical covariance converges (i.e., the quicker it decouples from the data), and the less signal loss we would expect to result.\relax }}{34}{figure.caption.17}}
\newlabel{fig:toy_sigloss16}{{2.4}{34}{The convergence level, as defined by Equation \eqref {eq:converge}, of empirically estimated covariances of mock EoR signals with different numbers of independent samples. In red, the mock EoR signal is comprised entirely of independent samples (100 of them). Subsequent colors show time-averaged signals. As the number of realizations increases, we see that the empirical covariances approach the true covariances. With more independent samples, the quicker an empirical covariance converges (i.e., the quicker it decouples from the data), and the less signal loss we would expect to result.\relax }{figure.caption.17}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.5}{\ignorespaces The convergence level, as defined by Equation \textup  {\hbox {\mathsurround \z@ \normalfont  (\ignorespaces \ref  {eq:converge_eig}\unskip \@@italiccorr )}}, of empirically estimated eigenvectors for different numbers of mock data realizations. The colors span from the 0th eigenmode (has the highest eigenvalue) to the 19th eigenmode (has the lowest eigenvalue), where they are ordered by eigenvalue in descending order. This figure shows that the zeroth eigenmode converges the quickest, implying that eigenvectors with eigenvalues that are substantially different than the rest (the FG-dominated mode has a much higher eigenvalue than the EoR modes) are able to converge to the true eigenvectors the quickest. On the other hand, eigenmodes $1$-$19$ have similar eigenvalues and are slower to converge because of degeneracies between them.\relax }}{35}{figure.caption.18}}
\newlabel{fig:toy_sigloss17}{{2.5}{35}{The convergence level, as defined by Equation \eqref {eq:converge_eig}, of empirically estimated eigenvectors for different numbers of mock data realizations. The colors span from the 0th eigenmode (has the highest eigenvalue) to the 19th eigenmode (has the lowest eigenvalue), where they are ordered by eigenvalue in descending order. This figure shows that the zeroth eigenmode converges the quickest, implying that eigenvectors with eigenvalues that are substantially different than the rest (the FG-dominated mode has a much higher eigenvalue than the EoR modes) are able to converge to the true eigenvectors the quickest. On the other hand, eigenmodes $1$-$19$ have similar eigenvalues and are slower to converge because of degeneracies between them.\relax }{figure.caption.18}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.6}{\ignorespaces Our ``fringe-rate filtered" (time-averaged) toy model dataset. We average every four samples together, yielding $25$ independent samples in time. Real parts are shown here.\relax }}{35}{figure.caption.19}}
\newlabel{fig:toy_sigloss5}{{2.6}{35}{Our ``fringe-rate filtered" (time-averaged) toy model dataset. We average every four samples together, yielding $25$ independent samples in time. Real parts are shown here.\relax }{figure.caption.19}{}}
\citation{parsons_et_al2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.2.4}Other Weighting Options}{36}{subsection.2.2.4}}
\newlabel{sec:otherweight}{{2.2.4}{36}{Other Weighting Options}{subsection.2.2.4}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.7}{\ignorespaces Resulting power spectrum estimate for the ``fringe-rate filtered" (time-averaged) toy model simulation --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). We use empirically estimated inverse covariance weighting where $\textbf  {C}$ is computed from the data. There is a larger amount of signal loss than for the non-averaged data, a consequence of weighting by eigenmodes that are more strongly coupled to the data due to there being fewer independent modes in the data.\relax }}{37}{figure.caption.20}}
\newlabel{fig:toy_sigloss7}{{2.7}{37}{Resulting power spectrum estimate for the ``fringe-rate filtered" (time-averaged) toy model simulation --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). We use empirically estimated inverse covariance weighting where $\textbf {C}$ is computed from the data. There is a larger amount of signal loss than for the non-averaged data, a consequence of weighting by eigenmodes that are more strongly coupled to the data due to there being fewer independent modes in the data.\relax }{figure.caption.20}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.8}{\ignorespaces Resulting power spectra estimates for our ``fringe-rate filtered" (time-averaged) toy model simulation --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). We show four alternate weighting options that each minimize signal loss, including modeling the covariance matrix of EoR (upper left), regularizing $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \textbf  {C}$}\mathaccent "0362{\textbf  {C}}$ by adding an identity matrix to it (upper right), using only the first three eigenmodes of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \textbf  {C}$}\mathaccent "0362{\textbf  {C}}$ (lower left), and keeping only the diagonal elements of $\setbox \z@ \hbox {\frozen@everymath \@emptytoks \mathsurround \z@ $\textstyle \textbf  {C}$}\mathaccent "0362{\textbf  {C}}$ (lower right). The first case (upper left) is not feasible in practice since we do not know $\textbf  {C}_{\rm  FG}$ and $\textbf  {C}_{\rm  EoR}$ like we do in the toy model.\relax }}{38}{figure.caption.21}}
\newlabel{fig:toy_sigloss8}{{2.8}{38}{Resulting power spectra estimates for our ``fringe-rate filtered" (time-averaged) toy model simulation --- foregrounds only (blue), EoR only (red), and the weighted FG + EoR dataset (green). We show four alternate weighting options that each minimize signal loss, including modeling the covariance matrix of EoR (upper left), regularizing $\widehat {\textbf {C}}$ by adding an identity matrix to it (upper right), using only the first three eigenmodes of $\widehat {\textbf {C}}$ (lower left), and keeping only the diagonal elements of $\widehat {\textbf {C}}$ (lower right). The first case (upper left) is not feasible in practice since we do not know $\textbf {C}_{\rm FG}$ and $\textbf {C}_{\rm EoR}$ like we do in the toy model.\relax }{figure.caption.21}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.3}Signal Loss Mathematical Framework}{39}{section.2.3}}
\citation{liu_tegmark2011}
\citation{trott_et_al2012}
\citation{liu_et_al2014b}
\citation{dillon_et_al2014}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.1}A Toy Model for Inverse Covariance Weighting}{40}{subsection.2.3.1}}
\newlabel{sec:icw_appendix}{{2.3.1}{40}{A Toy Model for Inverse Covariance Weighting}{subsection.2.3.1}{}}
\newlabel{eq:IdealToyModelCovariance}{{2.16}{40}{A Toy Model for Inverse Covariance Weighting}{equation.2.3.16}{}}
\newlabel{eq:IdealToyModelEstimator}{{2.17}{40}{A Toy Model for Inverse Covariance Weighting}{equation.2.3.17}{}}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.3.2}A Toy Model for Signal Loss}{41}{subsection.2.3.2}}
\newlabel{sec:sigloss_appendix}{{2.3.2}{41}{A Toy Model for Signal Loss}{subsection.2.3.2}{}}
\newlabel{eq:phatloss}{{2.28}{42}{A Toy Model for Signal Loss}{equation.2.3.28}{}}
\newlabel{eq:ChatDef}{{2.30}{42}{A Toy Model for Signal Loss}{equation.2.3.30}{}}
\citation{liu_tegmark2011}
\citation{das_et_al2011a}
\newlabel{eq:phatlossexpanded}{{2.33}{43}{A Toy Model for Signal Loss}{equation.2.3.33}{}}
\@writefile{toc}{\contentsline {section}{\numberline {2.4}Error Estimation Toy Model}{43}{section.2.4}}
\newlabel{sec:ErrorOverview}{{2.4}{43}{Error Estimation Toy Model}{section.2.4}{}}
\citation{efron_tibshirani1994}
\citation{andrae2010}
\@writefile{lof}{\contentsline {figure}{\numberline {2.9}{\ignorespaces Error estimation from bootstrapping as a function of the number of elements drawn per bootstrap when sampling with replacement. The star represents the standard deviation of $N_{\rm  boot}=500$ bootstraps, each created by drawing $1000$ elements (with replacement) from a length $1000$ array of a Gaussian random signal. The black points correspond to time-averaged data (correlated data) which has $100$ independent samples. They illustrate how errors can be under-estimated if drawing more elements than there are independent samples in the data. The estimated errors match up with the theoretical prediction only at $N=100$.\relax }}{45}{figure.caption.22}}
\newlabel{fig:toy_error1}{{2.9}{45}{Error estimation from bootstrapping as a function of the number of elements drawn per bootstrap when sampling with replacement. The star represents the standard deviation of $N_{\rm boot}=500$ bootstraps, each created by drawing $1000$ elements (with replacement) from a length $1000$ array of a Gaussian random signal. The black points correspond to time-averaged data (correlated data) which has $100$ independent samples. They illustrate how errors can be under-estimated if drawing more elements than there are independent samples in the data. The estimated errors match up with the theoretical prediction only at $N=100$.\relax }{figure.caption.22}{}}
\citation{ali_et_al2015}
\citation{petrovic_and_oh2011}
\@writefile{toc}{\contentsline {section}{\numberline {2.5}Bias Toy Model}{46}{section.2.5}}
\newlabel{sec:BiasOverview}{{2.5}{46}{Bias Toy Model}{section.2.5}{}}
\citation{vedantham_et_al2012}
\citation{chapman_et_al2012}
\citation{parsons_et_al2012a}
\citation{parsons_et_al2012b}
\citation{dillon_et_al2013a}
\citation{wang_et_al2013}
\citation{parsons_et_al2014}
\citation{liu_et_al2014a}
\citation{wolz_et_al2014}
\citation{liu_et_al2014b}
\citation{dillon_et_al2015}
\citation{pober_et_al2016a}
\citation{trott_et_al2016}
\citation{datta_et_al2010,parsons_et_al2012b,vedantham_et_al2012,pober_et_al2013,thyagarajan_et_al2013,liu_et_al2014a,liu_et_al2014b,patil_et_al2017}
\citation{ali_et_al2015}
\citation{parsons_et_al2014}
\citation{dillon_et_al2014}
\citation{dillon_et_al2015}
\citation{jacobs_et_al2015}
\citation{beardsley_et_al2016}
\citation{trott_et_al2016}
\citation{ewall-wice_et_al2017,kerrigan_et_al2018}
\citation{liu_et_al2014b}
\citation{ali_et_al2015}
\citation{ali_et_al2015}
\citation{parsons_et_al2014}
\citation{dillon_et_al2014}
\citation{beardsley_et_al2016}
\citation{patil_et_al2017}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.1}Foreground and Noise Bias}{47}{subsection.2.5.1}}
\newlabel{sec:BiasTypes}{{2.5.1}{47}{Foreground and Noise Bias}{subsection.2.5.1}{}}
\citation{dillon_et_al2014}
\citation{parsons_et_al2014}
\citation{quenouille1949}
\citation{tukey1958}
\citation{keating_et_al2016}
\@writefile{toc}{\contentsline {subsection}{\numberline {2.5.2}Jackknife Tests}{48}{subsection.2.5.2}}
\newlabel{sec:JackknifeOverview}{{2.5.2}{48}{Jackknife Tests}{subsection.2.5.2}{}}
\newlabel{eq:bias1}{{2.36}{49}{Jackknife Tests}{equation.2.5.36}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.10}{\ignorespaces A null jackknife test shown as the power spectrum difference between two measurements (black), compared to the power spectrum of noise alone (green). Because the null test is not consistent with noise, it suggests the presence of a systematic in either $\textbf  {x}_{1}$ or $\textbf  {x}_{2}$. Null tests of clean measurements should be consistent with thermal noise.\relax }}{50}{figure.caption.23}}
\newlabel{fig:toy_bias1}{{2.10}{50}{A null jackknife test shown as the power spectrum difference between two measurements (black), compared to the power spectrum of noise alone (green). Because the null test is not consistent with noise, it suggests the presence of a systematic in either $\textbf {x}_{1}$ or $\textbf {x}_{2}$. Null tests of clean measurements should be consistent with thermal noise.\relax }{figure.caption.23}{}}
\@writefile{lof}{\contentsline {figure}{\numberline {2.11}{\ignorespaces Power spectrum estimates for $\textbf  {x}_{1}$ and $\textbf  {x}_{2}$, two jackknives of the toy model. They suggest the presence of a systematic in $\textbf  {x}_{2}$ only, illustrating how jackknives can be used to tease out excesses. Clean measurements should remain consistent despite the jackknife taken.\relax }}{50}{figure.caption.24}}
\newlabel{fig:toy_bias2}{{2.11}{50}{Power spectrum estimates for $\textbf {x}_{1}$ and $\textbf {x}_{2}$, two jackknives of the toy model. They suggest the presence of a systematic in $\textbf {x}_{2}$ only, illustrating how jackknives can be used to tease out excesses. Clean measurements should remain consistent despite the jackknife taken.\relax }{figure.caption.24}{}}
\@setckpt{methods_PS/methods_PS}{
\setcounter{page}{52}
\setcounter{equation}{41}
\setcounter{enumi}{0}
\setcounter{enumii}{0}
\setcounter{enumiii}{0}
\setcounter{enumiv}{0}
\setcounter{footnote}{1}
\setcounter{mpfootnote}{0}
\setcounter{part}{0}
\setcounter{chapter}{2}
\setcounter{section}{5}
\setcounter{subsection}{2}
\setcounter{subsubsection}{0}
\setcounter{paragraph}{0}
\setcounter{subparagraph}{0}
\setcounter{figure}{11}
\setcounter{table}{0}
\setcounter{NAT@ctr}{0}
\setcounter{Item}{0}
\setcounter{Hfootnote}{1}
\setcounter{bookmark@seq@number}{33}
\setcounter{parentequation}{0}
\setcounter{ContinuedFloat}{0}
\setcounter{r@tfl@t}{0}
\setcounter{section@level}{2}
}
